
spring:
  profiles:
    active: nonprod
---
spring:
  profiles: dev
  data.mongodb:
    uri:mongodb://172838290460:cMzKPKDt@ec2-54-218-74-242.us-west-2.compute.amazonaws.com/tweetappdb
      socketTimeout:20000
---
spring:
  profiles: nonprod
  data.mongodb:
    uri:mongodb://172838290460:cMzKPKDt@ec2-54-218-74-242.us-west-2.compute.amazonaws.com/tweetappdb
      socketTimeout:20000
---
spring:
  profiles: prod
  data.mongodb:
    uri:mongodb://172838290460:cMzKPKDt@ec2-54-218-74-242.us-west-2.compute.amazonaws.com/tweetappdb
      socketTimeout:20000
---
server:
  port : 8090
---
spring:
   kafka:
     consumer:
        bootstrap-servers: localhost:9092
        group-id: group_id
        auto-offset-reset: earliest
        key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
     producer:
        bootstrap-servers: localhost:9092
        key-serializer: org.apache.kafka.common.serialization.StringSerializer
        value-serializer: org.apache.kafka.common.serialization.StringSerializer
---
logging:
  level:
    org.springframework: ERROR
    com.tweetapp: INFO
  pattern:
    console: "%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"
    file: "%d %p %c{1.} [%t] %m%n"
  logging.file.name: app.log
  
  
management:
  endpoints:
    web:
      exposure:
        include:
          - prometheus
          - health
          - info
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

scrape_configs:
  - job_name: 'spring boot scrape'
    metrics_path: '/actuator/prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:8080']
 